import time
import torch
import wandb
import os
import numpy as np
from bidexhands.algorithms.marl.utils.shared_buffer import SharedReplayBuffer
from bidexhands.algorithms.marl.ExIWoL_policy import ExIWoL_Policy
from bidexhands.algorithms.marl.ExIWoL_trainer import ExIWoL_Trainer


def _t2n(x):
    return x.detach().cpu().numpy()


class ExIWoL_Runner:
    def __init__(self, vec_env, config, model_dir=""):
        self.envs = vec_env
        self.eval_envs = vec_env

        # parameters
        config['episode_length'] = vec_env.task.cfg["env"]["episodeLength"]
        self.env_name = vec_env.task.cfg["env"]["env_name"]
        self.algorithm_name = config["algorithm_name"]
        self.experiment_name = config["experiment_name"]
        self.use_centralized_V = config["use_centralized_V"]
        self.use_obs_instead_of_state = config["use_obs_instead_of_state"]
        self.num_env_steps = config["num_env_steps"]
        self.episode_length = vec_env.task.cfg["env"]["episodeLength"]
        self.n_rollout_threads = config["n_rollout_threads"]
        self.n_eval_rollout_threads = config["n_rollout_threads"]
        self.use_linear_lr_decay = config["use_linear_lr_decay"]
        self.hidden_size = config["hidden_size"]
        self.use_render = config["use_render"]
        self.recurrent_N = config["recurrent_N"]

        # interval
        self.save_interval = config["save_interval"]
        self.use_eval = config["use_eval"]
        self.eval_interval = config.get("eval_interval", 100)
        self.eval_episodes = config["eval_episodes"]
        self.log_interval = config["log_interval"]

        self.seed = self.envs.task.cfg["seed"]
        self.model_dir = model_dir

        self.num_agents = self.envs.num_agents
        self.device = f"cuda:{self.envs.task.device_id}" if "cuda" in str(self.envs.task.device) else "cpu"
        print(f'Using device in runner.py: {self.device}')

        torch.autograd.set_detect_anomaly(True)
        torch.backends.cudnn.enabled = True
        torch.backends.cudnn.benchmark = True

        self.run_dir = config["run_dir"]
        self.log_dir = str(
            self.run_dir + '/' + self.env_name + '/' + self.algorithm_name + '/logs_seed{}'.format(self.seed))
        if not os.path.exists(self.log_dir):
            os.makedirs(self.log_dir)

        # Initialize wandb
        wandb.init(
            project=f"dexterousHands",
            name=f"{self.algorithm_name}_seed{self.seed}",
            config={
                "algorithm": self.algorithm_name,
                "env_name": self.env_name,
                "num_agents": self.num_agents,
                "num_env_steps": self.num_env_steps,
                "episode_length": self.episode_length,
                "n_rollout_threads": self.n_rollout_threads,
                "seed": self.seed,
                "use_centralized_V": self.use_centralized_V,
            }
        )

        self.save_dir = str(
            self.run_dir + '/' + self.env_name + '/' + self.algorithm_name + '/models_seed{}'.format(self.seed))
        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)

        # policy network
        self.policy = ExIWoL_Policy(config,
                                   self.envs.obs_space[0],
                                   self.envs.share_observation_space[0],
                                   self.envs.act_space[0],
                                   device=self.device)

        if self.model_dir != "":
            self.restore()

        # algorithm
        self.trainer = ExIWoL_Trainer(config, self.policy, device=self.device)

        # buffer
        self.buffer = SharedReplayBuffer(config,
                                         self.num_agents,
                                         self.envs.obs_space[0],
                                         self.envs.share_observation_space[0],
                                         self.envs.act_space[0],
                                         self.device)

    def run(self):
        self.warmup()
        start = time.time()
        episodes = int(self.num_env_steps) // self.episode_length // self.n_rollout_threads

        for episode in range(episodes):
            if self.use_linear_lr_decay:
                self.trainer.policy.lr_decay(episode, episodes)

            for step in range(self.episode_length):
                # Sample actions
                values, actions, action_log_probs, rnn_states, rnn_states_critic, actions_env = self.collect(step)

                # Observe reward and next obs
                obs, share_obs, rewards, dones, infos, _ = self.envs.step(actions_env)

                data = obs, share_obs, rewards, dones, infos, \
                       values.detach(), actions, action_log_probs.detach(), \
                       rnn_states.detach(), rnn_states_critic.detach()

                # insert data into buffer
                self.insert(data)

            # compute return and update network
            self.compute()
            train_infos = self.train()

            total_num_steps = (episode + 1) * self.episode_length * self.n_rollout_threads

            # save model
            if (episode % self.save_interval == 0 or episode == episodes - 1):
                self.save()

            # log information
            if episode % self.log_interval == 0:
                end = time.time()
                print("\nAlgo {} Exp {} updates {}/{} episodes, total num timesteps {}/{}, FPS {}.\n"
                      .format(self.algorithm_name,
                              self.experiment_name,
                              episode,
                              episodes,
                              total_num_steps,
                              self.num_env_steps,
                              int(total_num_steps / (end - start))))

                self.log_train(train_infos, total_num_steps)

                aver_episode_rewards = torch.mean(self.buffer.rewards).item() * self.episode_length
                print("some episodes done, average rewards: ", aver_episode_rewards)
                wandb.log({"train_episode_rewards": aver_episode_rewards}, step=total_num_steps)

            # eval
            if episode % self.eval_interval == 0 or episode == episodes - 1:
                self.eval(total_num_steps)

    def warmup(self):
        # reset env
        obs, share_obs, _ = self.envs.reset()
        # share_obs = obs
        if isinstance(share_obs, np.ndarray):
            share_obs = torch.from_numpy(share_obs).to(self.device)
        if isinstance(obs, np.ndarray):
            obs = torch.from_numpy(obs).to(self.device)

        print(obs.shape)
        print(share_obs.shape)

        # replay buffer
        self.buffer.share_obs[0].copy_(share_obs)
        self.buffer.obs[0].copy_(obs)

    def collect(self, step):
        """Collect rollouts for training."""
        # Get actions from policy
        values, actions, action_log_probs, rnn_states, rnn_states_critic = self.trainer.policy.get_actions(
            self.buffer.share_obs[step].reshape(self.n_rollout_threads * self.num_agents, -1),
            self.buffer.obs[step].reshape(self.n_rollout_threads * self.num_agents, -1),
            self.buffer.rnn_states[step].reshape(self.n_rollout_threads * self.num_agents, self.recurrent_N, -1),
            self.buffer.rnn_states_critic[step].reshape(self.n_rollout_threads * self.num_agents, self.recurrent_N, -1),
            self.buffer.masks[step].reshape(self.n_rollout_threads * self.num_agents, -1),
        )

        # Ensure actions has the correct shape [n_rollout_threads, num_agents, action_dim]
        if actions.dim() == 1:
            actions = actions.reshape(self.n_rollout_threads, self.num_agents, -1)
        elif actions.dim() == 2:
            actions = actions.reshape(self.n_rollout_threads, self.num_agents, -1)

        values = values.reshape(self.n_rollout_threads, self.num_agents, -1)
        action_log_probs = action_log_probs.reshape(self.n_rollout_threads, self.num_agents, -1)
        rnn_states = rnn_states.reshape(self.n_rollout_threads, self.num_agents, self.recurrent_N, -1)
        rnn_states_critic = rnn_states_critic.reshape(self.n_rollout_threads, self.num_agents, self.recurrent_N, -1)

        actions_env = actions.reshape(self.n_rollout_threads, self.num_agents * actions.shape[-1])

        # Ensure actions_env is contiguous in memory
        actions_env = actions_env.contiguous()

        return values, actions, action_log_probs, rnn_states, rnn_states_critic, actions_env

    def insert(self, data):
        obs, share_obs, rewards, dones, infos, \
        values, actions, action_log_probs, rnn_states, rnn_states_critic = data

        # Convert dones to tensor if it's numpy array
        if isinstance(dones, np.ndarray):
            dones = torch.from_numpy(dones).to(self.device).reshape(self.n_rollout_threads, self.num_agents, -1)

        # Convert obs and share_obs to tensors if they are numpy arrays
        if isinstance(obs, np.ndarray):
            obs = torch.from_numpy(obs).to(self.device)
        if isinstance(share_obs, np.ndarray):
            share_obs = torch.from_numpy(share_obs).to(self.device)

        # Convert rewards to tensor if it's numpy array
        if isinstance(rewards, np.ndarray):
            rewards = torch.from_numpy(rewards).to(self.device)

        mask = dones.squeeze(-1).bool()  # (threads, agents) 2â€‘D
        rnn_states[mask] = 0
        rnn_states_critic[mask] = 0

        masks = torch.ones(self.n_rollout_threads, self.num_agents, 1, device=self.device)
        masks[dones == True] = 0

        self.buffer.insert(share_obs, obs, rnn_states, rnn_states_critic,
                           actions, action_log_probs, values, rewards, masks)

    @torch.no_grad()
    def compute(self):
        self.trainer.prep_rollout()
        next_values = self.trainer.policy.get_values(
            self.buffer.obs[-1].reshape(self.n_rollout_threads * self.num_agents, -1),
            self.buffer.rnn_states_critic[-1].reshape(self.n_rollout_threads * self.num_agents, self.recurrent_N, -1),
            self.buffer.masks[-1].reshape(self.n_rollout_threads * self.num_agents, -1))
        next_values = next_values.reshape(self.n_rollout_threads, self.num_agents, -1).detach()
        self.buffer.compute_returns(next_values, self.trainer.value_normalizer)

    def train(self):
        self.trainer.prep_training()
        train_infos = self.trainer.train(self.buffer)
        self.buffer.after_update()
        return train_infos

    def save(self):
        policy_actor = self.trainer.policy.actor
        torch.save(policy_actor.state_dict(), str(self.save_dir) + "/actor.pt")
        policy_critic = self.trainer.policy.critic
        torch.save(policy_critic.state_dict(), str(self.save_dir) + "/critic.pt")
        if self.trainer._use_valuenorm:
            policy_vnorm = self.trainer.value_normalizer
            torch.save(policy_vnorm.state_dict(), str(self.save_dir) + "/vnorm.pt")

    def restore(self):
        policy_actor_state_dict = torch.load(str(self.model_dir) + '/actor.pt')
        self.policy.actor.load_state_dict(policy_actor_state_dict)
        if not self.use_render:
            policy_critic_state_dict = torch.load(str(self.model_dir) + '/critic.pt')
            self.policy.critic.load_state_dict(policy_critic_state_dict)
            if self.trainer._use_valuenorm:
                policy_vnorm_state_dict = torch.load(str(self.model_dir) + '/vnorm.pt')
                self.trainer.value_normalizer.load_state_dict(policy_vnorm_state_dict)

    def log_train(self, train_infos, total_num_steps):
        for k, v in train_infos.items():
            wandb.log({k: v}, step=total_num_steps)

    def log_env(self, env_infos, total_num_steps):
        for k, v in env_infos.items():
            if isinstance(v, torch.Tensor):
                wandb.log({k: torch.mean(v).item()}, step=total_num_steps)
            else:
                wandb.log({k: v}, step=total_num_steps)

    @torch.no_grad()
    def eval(self, total_num_steps):
        desired_episodes = self.n_eval_rollout_threads
        eval_total_episodes = 0
        episode_rewards = torch.zeros(
            self.n_eval_rollout_threads, self.num_agents, device=self.device
        )
        eval_episode_rewards = []

        obs, _, _ = self.eval_envs.reset()
        if isinstance(obs, np.ndarray):
            obs = torch.from_numpy(obs).to(self.device)

        rnn_states = torch.zeros(
            self.n_eval_rollout_threads, self.num_agents,
            self.recurrent_N, self.hidden_size, device=self.device
        )
        masks = torch.ones(self.n_eval_rollout_threads, self.num_agents, 1, device=self.device)

        while eval_total_episodes < desired_episodes:
            self.trainer.prep_rollout()
            actions, rnn_states = self.trainer.policy.act(obs.reshape(self.n_rollout_threads * self.num_agents, -1),
                                                          rnn_states.reshape(self.n_rollout_threads * self.num_agents,
                                                                             self.recurrent_N, -1),
                                                          masks.reshape(self.n_rollout_threads * self.num_agents, -1),
                                                          deterministic=True)

            actions_env = actions.reshape(self.n_rollout_threads, self.num_agents * actions.shape[-1])
            obs, _, rewards, dones, infos, _ = self.eval_envs.step(actions_env)

            # Convert numpy arrays to tensors
            if isinstance(obs, np.ndarray):
                obs = torch.from_numpy(obs).to(self.device)
            if isinstance(rewards, np.ndarray):
                rewards = torch.from_numpy(rewards).to(self.device)
            if isinstance(dones, np.ndarray):
                dones = torch.from_numpy(dones).to(self.device)

            episode_rewards += rewards.reshape(self.n_eval_rollout_threads, self.num_agents)
            done_mask = torch.all(dones.squeeze(-1), dim=1)

            if done_mask.any():
                finished_ids = done_mask.nonzero(as_tuple=False).squeeze(-1)

                eval_episode_rewards.extend(
                    episode_rewards[finished_ids].clone().cpu()
                )
                eval_total_episodes += len(finished_ids)

                rnn_states[finished_ids].zero_()
                episode_rewards[finished_ids].zero_()

        eval_episode_rewards = torch.stack(eval_episode_rewards)
        mean_r = eval_episode_rewards.mean()
        max_r = eval_episode_rewards.max()

        info = dict(
            eval_average_episode_rewards=mean_r,
            eval_max_episode_rewards=max_r,
            eval_total_episodes=eval_total_episodes,
        )
        print(f"[Eval] {info}")
        self.log_env(info, total_num_steps)